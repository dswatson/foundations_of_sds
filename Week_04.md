# Week 4: Algorithmic Explainability

## Discussion Prompts/Questions

All that matters is performance. If model $f_1$ outperforms model $f_2$, then who cares which is more "interpretable"?


* It depends on how low/high stakes the problem is. As Rudin argues in her paper, if it is a low-stakes problem (like Amazon recommendations), interpretability is not as life-or-death, so I guess we can "care" less. However, for a more high-stakes situation, like risk-assessment score algorithm, it is imperative that the model is interpretable. This is because in these high-stake decisions where humans are using machines to make decisions, in order to be disputable, the model needs to be human-interpretable.
* But--by the same token, doesn't the "high stakes" issue cut both ways? If lives hang in the balance, we certainly don't want to go with the suboptimal predictor!
* The context in which the model is being deployed matters. It is not necessarily only a matter of whether it is a high-stakes decision, but also if the value of interpretability exceeds the value of accuracy in that situation. For example, self-driving algorithms are a high-stakes decision (because if it doesnâ€™t work, cars will get into accidents), but here accuracy trumps interpretability.
* It depends on the problem at hand. In some cases, a high accuracy may be a matter of life and death and could not be achieved with a more interpretable statistical method. On the other hand, if a model would be used in the policy making sphere, an uninterpretable model is also very likely to create confusion and implementation problems, regardless of how accurate it may be. In such cases, it would be in the general interest to keep things interpretable.
* It depends on the case. If a solution is used in areas where it can change lives of people then accuracy and transparency are significant. However, in some cases where people's lives are not affected by this then we can use less transparent models in order to get the most accurate result.
* Well, the criteria for a good model depends on the goal of model estimation. If it is realised in industry and all you want is maximum accuracy (AUC, precision, etc.) then it might be fine to use perfomance criteria, especially when the price of potential mistakes is not too high. However, if the model aims to test theoretical hypotheses and just gives highest perfomance but represents a black box than it is useless. 
* For whom is it "outperforming"? It might be the better model as a whole, but still discriminate systematically against certain gorups.
* But is it auditable? At least in the public sector, many governments have pretty explicit rules on appeals and audit processes, which we argue are important for holding government accountable. 
* Essentially there are two sides to the accuracy coin. Assuming the performance is equal = 60% accuracy, then the data points which can't be explained by the model can be our focus. If we don't know WHY these mispredictions occur, then interpretability does matter. No model is 100% accuracy on all possible testing data, so its the mispredictions not just the correct predictions which matter (but are often overlooked in favour of reporting one overhead accuracy metric). 
* Interpretability allows for accounting for mistakes of parts of the model, i.e. if some model predicts poorly, then you can look into the model in order to find out why that is? This can be especially helpful if we apply a certain model on a different (new) environment.
* If both models are only accurate for certain classes then a more interpretable model might help identify these discrepancies -> accuracy doesn't capture uneven performance for different groups of people.
* Rudin says that it a myth. There isn't necessarily a trade-off between accuracy and interpretability. Another important concern is if there are data protections in place. I think that should be a consideration in addition to the value of the model itself. Need to put people before numbers. 
* How bad is it if we get it wrong, versus how much do we feel we are owed an explanation? In the case of a bail algorithm (especially for non-violent crimes, where we're not worried about someone potentially murdering/stealing/etc while out on bail), where we're taking away someone's liberty, I think they deserve an explanation for why--and if we get it wrong, less is at stake (of course, this calculation might change for crimes like murder or rape--especially in cases of serial alleged offenders). What about the case where something is high-stakes but doesn't have as much to do with humans--like using machine learning to decide which parts of an airplane need to be further inspected versus which can be approved? I'd argue performance is more important in this case (except insofar as a mistake might lead to people being upset that we handed over decision making to an algorithm and cause a backlash).
* Model 'interpretability' is important to bridge information asymmetry, which is necessary to evaluate the model holistically; otherwise the model remains only comprehensible by a certain group of individuals (e.g. techno-elites), and becomes more skewed towards certain heuristics / biases unknown to the layman.
* Interpretability is still crucial for domain-specific aplications where there is asymmetric information between experts in the field (i.e., law, medicine) to monitor and assess decisions, as it is more subjective in nature.
* For socially consequential predictions, interpretability increases the usefulness of a model; if the model is a black-box its use would be limited even if its performance were impressive; for example in applications in areas such as medicine, a model that is not easily interpreted might not be used because of the high stakes involved in case the algorithm fails.

* Accountability relies on interpretability.


What makes an explanation (algorithmic or otherwise) successful?
* Can be explained to a child (i.e., in human-intelligeble, simple terms).
* It has clear practical implications.
* Input - Throughput - Output > giving somebody the ability to connect the dots builds comfort and gives a sense of control.
* Can be successfully challenged? 
* Understanding of the effect of specific variables.
* Counterfactual evidence possibility.
* Can be discussed to upgrade algorythms in future.
* Stability of the explanation - it can be applied to similar instances of the same problem, for example when seeking to understand why individuals of differnt backgrounds were given a specific result we will be provided with the same reasoning framework. 
* That it corresponds to what is expected of an explanation in its specific setting. 
* The explanation predicts the outcome of the algorithm to a specified threshold (that threshold depends on the context).
* Understandable by the end user (tailored based on the individual's belief and level of expertise) + an explanation that is not borne out of the algorithm, but instead, one that is used to structure that algorithm. 
* Miller (2017) says: An explanation is an answer to a **WHY** question: e.g. see below BUT I think a counterfactural of **'Why A not B'** or **Why X if input = A, why Y if input = B** is a more powerful why question.
    - Why did not the treatment work on the patient? alternative: Why did the treatment work on patient A and not on patient B? 
    - Why have we not been contacted by alien life yet? alternative: In what counterfactual worlds would we have been contacted by alien life and what is different about this world. 
    - A constrastive explanation exposes both the presence and lack of deciding features e.g. decision tree gives you all the branches of "No"s and "Yes"s. 
* Domain Specific - A good explanation would have 3-5 levels of complexity depending on domain specific understanding e.g. the 'Professor', 'The Grad Student', 'The High Schooler', The Grandma' explanations. This prevents obscuring complexity from those that can understand, as a race to the bottom denominator, but also permits accesibility of explanation to people with less domain-specific understanding.
* Is the explanation an accurate and truthful representation of what is happening?
* Molnar: intepretability is the degree to which a human can undertand the cause of a decision. Explainability shouldn't just be understanding how inputs get mapped to outputs (LIME) but the understanding the underlying process for why that decision was made.
- A logical explanation that can be situated in the context of human decision making, not just how the algorithm was built / which data was used in the algorithm (i.e. linear model)
* I think that in the case of a binary/categorical decision, especially one where someone is being told yes/no (e.g., 'yes you do qualify for our bank account/lower cost insurance/etc'), counterfactuals are really important! People will be much more satisfied if they're told 'what they could have done better' or what would need to change for the decision to change.

Are linear models inherently intelligible? What about rule lists?

* Neither linear models nor rule lists allow us to escape complex interactions and high dimensionality. So where there are a small number of explanatory variables or small number of rules (say with a decision tree), we can construct a narrative in our heads (how we might define "inherently intelligible") but as vars/rules increase, the ability of humans to understand how certain explanatory variables may interact with one another or how, if you've reached a certain point in a decision tree, you got to that point, rather than another point in a decision tree diminishes.
* Any linear model is based on a set of assumptions. It simply does not work and deserves no interpretaion if those are not met at least approximately.
* Probably depends on the amount of explanatory variables here. You could conceive of a rule list with 100000 paramaters that would be impossible to read.
* I suppose they are for limited number of variables but at the same time they can't be used to explain complex problems.
* Yes, but they might foster naivite about the problem because everything gets flattend out on a straight line / pane. At some point, one needs to acknowledge that one can only do so much with modelling in non-linear terms. At some points researcher needs to acknowledge that the problem doesnt fit a simple abstraction.
* Judging by our statistics classes, they are not inherently intelligible, but they are definitely a helpful tool to understand correlations and/or interactions and are relatively more interpretable than black-box unsupervised ML techniques. However, linear models and rule lists potentially over-simplify what is happening in the real world, and thus while the immediate output may seem more intuitively intelligible, it could potentially foster false perceptions.
* I guess that they need to communicate with the relevant academic theories if we want them to be useful. Domain knowledge is really important and often overlooked.
* Endogenously linear models/rule lists are intelligible (just considering the model, its inputs, and its outputs). Exogenously (outside of the context of the model - extrapolating to society - does this still fit the definition of intelligible?), it may lack explaining general phenomena in the world outside the model.
* The more parameters in the model, the more difficult it is for the human brain to cognitively process those parameters and understand its implications.
* The multi-dimensional nature of some linear models do create barriers for explainability. For example, when a model, especially one in production that has a large number of features, begins introducing polynomial variables or interaction terms it becomes really difficult to explain it.
* Front-end output still dependent on back-end design - e.g. the data-cleaning / model selection decisions are obscured from the explanation. For example, if a decision tree has been pre-pruned or post-pruned to a max-depth then the final outputted decision tree incorporates a hidden decision - only interpretable if full pipeline is observed. 
* **How are we defining intelligible? What vs Why** / Is it the effect of a variable on another, or positing the real-world mechanism of HOW x causes y? If a linear model is chosen by step-wise algorithm - i.e. add all variables, and iteratively drop them to improve the fit - could be left with a set of variables that make no sense 'theoretically' - critics of automatic feature selection would advocate for theory first, model second. 
* Interesting human heuristic because of the name 'linear model' people assume this is easier to conceive in their height - we can visualise the traditional linear regression model taught in college stats classes - but actually almost impossible for human mind to visualise more than 3 dimensions - just as hard to 'inherently' interpretable as a neural net. 
* Yes, linear models and rule lists are relatively intelligible tools, but the technicalities of which depend on their scale and the level of statistical understanding amongst users (i.e. it is easy to understand and explain the direction of correlation between a covariate and response variable / if-else statements in classifications, but what if we scale up to incorporate transformations / interactions?)
* Yes to the extent that one can relatively easily understand the theory underpinning why a certain result was reached.
* No, as with all models the observer has to have a certain level of intuition based on experience to feel that they understand it.
* Linear models are more easily explainable in the sense that, unlike ML methods, they do not operate as black boxes; however that still depends on the way data was collected, cleaned, manipulated; entrenched biases can be difficult to spot and certain levels of complexity might still be uninteligible

## Reading Questions

[Burrell (2016)](http://journals.sagepub.com/doi/10.1177/2053951715622512) identifies three sources of algorithmic opacity: (1) corporate or state secrecy; (2) technical illiteracy; and (3) inherent complexity. Which of these do you think is most important and why? Are there any interesting relationships between the three?

* While 2 and 3 can be improved upon and made less opaque - #1 is the most dificult to bypass as we are not given an opportuntiy to investigate it. This is not to say that issues 2 and 3 will not lead to equally opaque algorithms, but that given enough education on the importance of decreasing algorithmic opacity they can be more readily remidied. 
* I find that #1 is the scariest, given that it could persist for forever. #2 and #3 seem like solveable issues with lots of backing behind them. If governments decide to use entirely technically interpretable models, they can still easily attempt to keep those explanations from the public (like COMPAS!).
* I'd argue that (1) is the most opaque because it is almost the first barrier to entry. We can improve technical literacy and complexity, but the public's agency/ability to improve corporate/state secrecy is very limited.
* I think that the dynamic of inherent complexity in combination with technical illiteracy and by extension also secrecy creates a very interesting yet scary power dynamic between ML researchers/companies and policy makers/citizens. Like in the case of economics, complexity often contributes to an image of perceived 'class'/'coolness' among policy makers.
* There is a way to overcome 2 and 3 source (get to know the stuff), but the first one means that you are consciously isolated from the information.
* (1) is significantly detrimental, but could possibly be regulated or governed (assuming functioning legislation or democracy). Personally I feel it is as hard to solve for (2) and (3). For (2), because of the normal distribution there will always be a limit as to how much people will be able to achieve the technical literacy required to understand enough how algorithms work. For (3), the way the field is approaching these problems currently it does not seem like we are able to fully unpack algorithmic complexity.
* One reason why the 3rd source of opacity is perhaps more important is that the other two have solutions that are somewhat more easily implemented (such as regulation, educational efforts) whereas it seems like there is no obvious way to easily solve the final issue and it could be tempting to replace it all together with one that is more transparent / doesn't work like a black box even if that sacrifices some of the accuracy.
* Depends where algorithms are primarily being developed - if hypothetical situ where all algos are open source then (1) is irrelevant, in other extreme if all algos are proprietary then (1) overrules any consideration of (2) or (3) because the algorithm can't be examined anyway. I think its difficult to make the argument that algorithms can't be considered IP - why can the recipe for ketchup be considered a corporate secret but algos can't? Could also stop algorithmic innovation if they have to be open source. There's a trade off, and likely demanding corporate revealing of algos is likely to damage small firms more than the big tech giants. Third party verified audits could balance this trade-off. 
* Why force companies to reveal properitary data used to make decisions when we didn't do so in the pre-algorithmic times? (Obviously, in some cases--like denying credit--we did do so, but in many other cases, we didn't). 
* The first point lends itself to be more easily scrutinized by social players. If opacity comes from points two or three, arguments on the societal level don't typically arise until that opacity leads to a sort of conflict of interest (cue COMPAS). The first point, however, plays into an ongoing discussion of how open governments/companies should be in all realms of information (financials, systems, etc.)
* (1) This is the only type of opacity that most obviously says something about the social structure it is a part of and that likely affects power dynamics.

## Rashomon, etc.

See [Rashomon](https://en.wikipedia.org/wiki/Rashomon)! Great movie. More relevantly, [Breiman (2001)](http://www2.math.uu.se/~thulin/mm/breiman.pdf) introduces the notion of a "Rashomon set", i.e. a class of models that all perform reasonably well on a given dataset. Rudin argues that in any given Rashomon set, there is always at least one intelligible model. If she is right, then there is no accuracy-interpretability tradeoff. 

For some more info on Rudin's work in this area, see her [SLIM](https://link.springer.com/article/10.1007/s10994-015-5528-6) and [CORELS](http://www.jmlr.org/papers/volume18/17-716/17-716.pdf) algorithms, as well as her [recent paper](https://arxiv.org/abs/1908.01755) on Rashomon curves and volumes. (Warning: very technical!)




