# Week 5: Privacy and Regulation in Data Science

Today's topics: 

1) Mechanism in the social sciences.
2) Philosophical starting points and major assumptions.

## Reading Questions

In this week’s readings, several texts discuss the relationship between privacy and democracy. Cohen (2012), for example, claims that the liberal self and the liberal democratic society are symbiotic ideals. More specifically, Cohen suggests that any society that permits the unchecked ascendancy of surveillance infrastructures cannot hope to remain a liberal democracy. Do you agree? Why or why not? In your answers, try to highlight the mechanisms that do (or do not) link privacy to democracy.

Why privacy is foundational to liberal democracy:
- According to Cohen, participation in a liberal democracy requires some "discomfort", to the extend the citizens feel the need to participate in government systems to fix societal issues. Cohen then says that the openness/lack of privacy found in social media and our latest tech advancements provides comfort/complacency, and therefore **takes away the citizen's sense of needing to participate in the government.  This participation is key to a liberal democracy**.

- Many of the individuals rights guranteed in a democracy stems from the belief of self-governance and self-determination. Surveillance can be seen as an infringement on those beliefs, thus contrary to the principles of democracy. Often the rationale for building a surveillance state (i.e. China) is that individual citizens are incapable of self-governance, and the state is able to best account for the interests of all which requires extensive surveillance. 
- **The idea of democracy is to some degree founded on the idea that autonomous members of demos together form the common social environment**. This means that the demos is sovereign and that it defines what is right by its own choices. If there is no privacy there is no space in which this process of definition can take place and what happens is that the myth of an autonomous body of individuals is uncovered, it becommes obvious that the wishes and choices of individuals are not in fact very free but predictable, and perhaps even controllable. Privacy therefore serves the nobel cause of upholding the myth of democracy. 

- Cohen argues that privacy permits the 'interplay' between the liberal self and the liberal democratic society, so privacy is foundational to liberal democracy in a sense that individual rights and freedoms are recognised by dictating the boundaries through which individuals are able to self-govern freely without infringement from external actors.

- **Citizens need to have 'space' to develop individual minds, which are necessary for a funcional liberal democracy**

* There's the classic Ben Franklin quote, "Those who would give up liberty for security deserve neither" (paraphrasing) which would argue that yes, this sacraficing of liberty in favor of more security from government means we are doomed. I think this is a pretty common argument these days too, but it seems unlikely to me that we'll reach a state where surveillance starts corrupting all of our liberal democracies. It seems more likely to me that a country that is already on track to destroy its liberal democracy is more likely to implement measures that get rid of privacy, rather than a loss of privacy bringing the downfall of a democracy. 

* The excessive liberty of one person / a group of people might curtail other people's liberty to exercise their demogratic rights. In result it comes down to the notion of demogracy operationalized in a particular society where surveillance might hence emerge as a vehicle to ensure that this very notion is not violated in favor of a greater common good.

* The power imbalance created by surveillance deters the formation and expression of non condorming views if individual rights are not guarded. Julie provides a useful framework for thinking of democracy as relying on pluralism, thus the attack on a diveristy of opions puts it at danger. 

* Privacy sort of sets a perimeter around the individual which protects them from too much influence from the state and protects their self determination. Liberal democracy depends on individuals in the society being able to express their will, thus privacy preserves the individual's capacity to develop that will freely.

* I agree with the keyword being 'unchecked'. I would agree that some level of surveillance may be necessary to ensure security however this must be limited to certain situations/curcumstances. The example that i give here is when it is necessary for police to search someones house, this invasion of privacy can only leagally take place after a judge has weighed the reasons for/against it and granted a search warrant. In this way the ideals of a liberal society are maintained whilst balancing the need for security.

Why privacy and liberal democracy are not neccessarily related: 
- Cohen argues privacy allows for the pluralism neccessary for democracy, but doesn't privacy also allow for illiberal/undemocratic pluralism to flourish (paradox of tolerance-esque)? **Seems like privacy can have both democratic and undemocratic effects, and the more important factor is why some individuals would become disenfranchised with democratic values**.

* Not necessarily. Just as total laizzez-faire market system might be argued to optimize information sharing and bargaining, in reality it would be a "In the long run we are all dead" scenario described by Kaynes. There should be multiple paths that link to improvement in privacy assurance, and democracy (which free market is often said to be an important component of it) would also have its "market failure" moment.

- Cohen seems to be placing personalization and privacy at odds with one another; as personalization increases, privacy decreases. As nice as personalization can be, it can also be harmful to democracy -- if you fail to experience those from different backgrounds (race, religion, political beliefs, education levels, etc.) in your everyday life -- which is becoming increasingly true (at least in the US and most likely in Europe as well) -- but your experience of the Internet becomes a bubble as well, then your views will most likely become more extreme, which is bad for pluralism -- which is in term damaging to democracy. This is what Facebook has been accused of -- even more interesting when you consider that Cohen wrote this article in 2013, long before FB had garnered the reputation it has today.

- Even in liberal democracy, privacy rights aren't extended to everyone and for all acts **e.g. criminal acts are usually exempt**.

- Proof by contradiction - privacy concerns still exist in autocratic states e.g. in China, it is not that citizens don't care about privacy outright, but that there is a differential relational concept of privacy e.g. government is often seen as the protector of citizens from third party company privacy violations - maybe more accurate to say liberal democracy and privacy **from government scruntiny** are linked. 

- It seems that Cohen implies that openness of technology/lack of privacy lends itself to creating a homogenous citizenry, and with a homogenous citizenry there's no urgency in participating in government.  However, I think that platforms which expose what may be considered private also exposes to heterogeneity of those who participate in the platform and allows people to consider new views they hadn't known about before, to process these new views in their own way, and to potentially contribute a novel view to the society they're part of.
 
- Under what context is privacy demanded for, to what extent, and by whom? Simple example: In certain circumstances such as in the act of voting, privacy is required for liberal democracy, because for it to work citizens need to be able to vote without fear or coercion. But in the context of say hiding criminal behaviour, it may not be antithetical to the principles of liberal democracy.


In ‘Big Data and the brave new world of social media research’, Schroeder (2014) suggests that Big Data can help to undermine the idea of personal responsibility. According to Schroeder, the very idea that our behavior can be predicted and manipulated goes against fundamental self-understandings of how society operates through individual decision-making. How does Schroeder’s argument relate to different assumptions regarding free will? Do you think that Big Data undermines human individuality and personal responsibility? Why or why not? 

Yes, big data can help undermine the idea of personal responsibility and self-determination: 

- **if recommendation algorithms can "nudge" agents into certain decision-making outcomes (via targeted and controlled manipulation), then decision-making becomes less autonomous** than had it not been influenced. Now, if big data allows those "nudges" to become more powerful, hence affecting decision-making outcomes to a greater extent, then it undermines the autonomy of agents even further. 

- Might be useful to consider recommender systems in the frame of Berlin's [two concepts of liberty ](https://plato.stanford.edu/entries/liberty-positive-negative/). Positive liberty implies ‘**individuals need to engage with a diversity of content in order to develop their critical faculties and realise their potential**’ and so in filtering content, RecSy violate positive freedom. BUT if we reach this conclusion, and consider a re-design of algorithms necessary for societal aims of diversity, privacy or cohension, then we tow the risky line between granting positive freedom without violating negative freedom (freedom from interferance from the state). For example, we still have to decide what is desirable for the algorithm to promote or not promote, and this decision by the state of what is acceptable is still an interference - and in the extreme, why is this different from e.g. the Chinese govt policy that recommender systems must promote pro-party content or censor anti-party content. Essentially, even if we realise big data/algorithmic approaches are wrong, their top-down redesign potentially violates freedoms too. 

* In a sense, predictions can frame the reality in which we are navigating (aka filter bubbles). Especially thinking about recommender systems, we might be misguided in way that cues we have once provided to a system will be treated as absolute truth and constantly reinforced through the predictions we recveive. **In way this might errode our ability to change or evolve our identity which became a function of free will an algorithmic co-determination**

* This viewpoint necessitates the consideration of whether we think we had free will before the advent of Big Data and what constitutes "individuality." In a way, **we could argue that individuals don’t ever have free will and are just a product of their communities and society**. (Perhaps this belongs in the category below?)

* A good example can be found in predictive modeling for medical decision making - in transfering responsibilty from the clinican to a model there is a shift in accountability for a diagnosis/course of treatment. Here both a legal and ethical challenge emerges as to how responsibility should assigned. On the contrary, one could also argue that the system challenges the physician to consider their decision more soundly if there is room for dialogue between the two, i.e. if deciding against a recommendation there would have to be serious consideration as to why. 

- We are generally not responsible for things that we are coerced to do. If there's such an asymmetry of information/knowledge between the people deploying algorithms and the people being exposed to them, then t**hese systems could be a form of coercion and would undermine personal responsibility**. 

- To an extent, big data algorithms undermines one's individuality especially when one's decision are massively compared to other's at a large scale, in terms of your attributes, background, etc. How does individuality fits in the 'accuracy' of the model. Like the insurance case, ultimately, one's background dictates which 'categories' they fit into

- Maybe? By that I mean, it will be very individual-dependent. Some more easily influenced individuals may be greatly affected by recommender systems, whereas strong willed individuals with a stronger sense of self-awarness may not be. So the presence of big data or recommender systems will have different degrees of impact on self-determination for different people. This makes the possible effects of big data difficult to state with absoluteness. 

- Yes, we and our social constructs of responsibility are of course shaped by what happens to the tructure of our social networks. This does not neccessarily make it problematic though, for one thing there might be counter forces. The idea of responcsibility will no matter what remain in some way, it might drastically change but some other way of structuring the social will innevitably take its place. The only thing that is problematic with this kind of change is our (perhaps justified) fear of change and unvillingness to make our moral axioms relative.

- Yes, **moreso self-determination than personal responsibility**. People have always been "identified" by the data they produce (by markets, by governments, by academic/work structures, etc.), but with "big data" that identity has become formalized. In other words, rather than being identified by the data produced by, say , word of mouth, which can change in the transfer of that data or be interpreted differently by different individuals, people are now identified by formal data which is less prone to changes in communication. There is less agency in self-determination than in personal responsibility - a person has the choice of the actions they take, but less choice in how they are identified.


No,big data does not change our fundamental self-understanding.

- **Free will has always been a convienent way of conceptualizing a black box model that we don't fully understand** (the human brain). It feels like we have free will, and the assumption is useful for predictions, so we keep it. I don't think this fundamentally changes with the advent of big data. We've always been suspectible to manipulation -- think of a politican making a speech to manipulate us, or even a friend trying to convince us to do something.  I also think it's relevant to see just how ineffective the knowledge brought by 'big data' really is -- Ralph makes the point that even a small fraction of behavior change is valuable to a company, since it's on such a large level, and that's true, but the reality is that it's still *not* very effective. Does it really make us do something we wouldn't otherwise do, or does it simply make our desires easier to realise? Maybe one day it will be more effective -- but you could say the same thing about research being carried out on political canvassing or communications in interpersonal relationships or cognitive behavioural therapy -- all attempt to find ways to manipulate/change human behaviour.  

- What is big data being used for? Big data does not necessarily change our fundamental self-understanding. The key is the purpose of its usage - why, how, by whom and for whom it is implemented. To cite an example, if it is being used like in China whereby an individual’s every small action may have an impact on their social credit rating, then it could undermine the idea of personal responsibility and self-determination. An an alternative example, the rise of the quantified self movement - in which we use sensors and trackers to track our personal data, e.g. health, sleep, time usage - may in fact have big data leading to more personal responsibility and powers of self-determination. 

- Depends. In the article, **Schroeder adopts the concept of 'technological determinism'** - i.e. the ability of technology to predict and manipulate individual behaviour, which he argues runs counter to self-understanding according to individual decision-making. However, it is entirely possible for behaviour to be manipulated while respecting freedom of choice (consider: liberal paternalism in Prospect Theory). For example, if identical sets of choices are presented to an individual, but the reference point is varied (e.g. opt-in vs opt-out systems), does this necessarily infringe on individual decision-making?

- It depends on the use of big data research. Ralph (2019) argued (along Weber's terms) that such research can lead to cages or exoskeletons, where exoskeletons would basically enable us to make more efficient decisions. So it is dependent on the underlying incentive mechanism behind big data research, and its use-cases, i.e., in this argument it is influencing human behavior.

* In some scenarios, we as humans are confronted by a burden of choice which can lead to decision paralysis. In these cases, big data might lend itself as a torch light to "de-complixify" situations to a level that we are capabable to make sound decisions in the first place.

* I agree from the point of view that if you do not raise this topic from such an angle, then in the future there may be bad consequences. In this regard, we can say that the fact that our behavior can be predicted and used for manipulation is important and dangerous for society. However, psychologists not on a large scale, but even before big data had the ability to do the same, but locally. Therefore, it is perhaps more important to also engage in teaching society to resist manipulation in the information space and to care about their own privacy by themselves. Moreover as it was said in Harvard Law Review liberal democratic citizenship requires a certain amount of discomfort — enough to motivate citizens to pursue improvements in the realization of social ideals.

* **I do not think that big data goes against our fundamental self-understanding. Even before technology as we know it, people were always trying to manipulate and predict each others' behaviors** (just now, with big data, it's just much better). That's what marketing and advertising is. Would even go as far as to argue that religion is a way of manipulating people to act in certain ways.
. 
* **I think having knowledge of these forces increases personal responsibility**, as it requires individuals to examine what forces at play are motivating their behavior.

* No. **Big Data simply reveals that our behavious as humans, though decided upon by free will is to an extent predictable**. Big Data can reveal that person 'y' is x% likely to do 'something' however it cannot say this with 100% certainty. 


* No, I'd like to take a compatibilist approach to free will here: "an agent’s action is free just in case the agent or manner in which the action is brought about is responsive to the reasons available to the agent at the time of action" - from Stanford's encyclopedia of philosophy. Whether or not someone's opinion or behavior can be predicted on an aggregate level does not affect our self-understanding as free agents if we believe that we arrived at this decision through rational deliberation.

### Unanswered food for thought...

Dai (2018) describes the emergence of social credit systems in China. A key mechanism discussed in the paper is the ‘reputation market’. Reputation, Dai suggests, is welfare-enhancing as it facilitates both economic and social interactions. Hence, reputational information is sought after and used by all kinds of rational actors to make decisions. Do you think that the analogy between economic markets and trust markets is helpful, dangerous, or both? What are the similarities and differences between how trust scores and reputational information would operate on a market? 